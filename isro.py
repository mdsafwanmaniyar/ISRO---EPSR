# -*- coding: utf-8 -*-
"""ISRO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-odmqBk_IptqWM5NAff5gAxJjL_Q9QpP

**Importing**
"""

import os
import numpy as np
import cv2
from glob import glob
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger

"""**SEEDING**

"""

os.environ ["PYTHONHASHSEED"] = str(42)
np.random.seed(42)
tf.random.set_seed(42)

from PIL import Image

image = Image.open('/content/non-aug/train/images/10.png')
width, height = image.size

print(f"Width: {width}, Height: {height}")

batch_size = 4
lr = 1e-4 ## 0.0001
epochs = 100
height = 400
width = 600

dataset_path = os.path.join('/content/non-aug')
files_dir = os.path.join("files", "non-aug")
model_file = os.path.join(files_dir, " unet-non-aug.h5")
log_file = os.path.join(files_dir, "log-non-aug.csv")

def create_dir(path):
  if not os.path.exists(path):
    os.makedirs (path)

create_dir(files_dir)

def conv_block(inputs, num_filters):
  x = Conv2D(num_filters, 3, padding="same") (inputs)
  x = BatchNormalization()(x)
  x = Activation("relu") (x)

  x = Conv2D(num_filters, 3, padding="same") (x)
  x = BatchNormalization()(x)
  x = Activation("relu")(x)
  return x

def encoder_block(inputs, num_filters):
  x = conv_block(inputs, num_filters)
  p = MaxPool2D((2, 2))(x)
  return x, p

def decoder_block(inputs, skip, num_filters):
  x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(inputs)
  x = Concatenate()([x, skip])
  x = conv_block(x, num_filters)
  return x

from tensorflow.keras.layers import Conv2D, Input, UpSampling2D, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, Input, UpSampling2D, Concatenate, MaxPooling2D, Cropping2D
import tensorflow as tf
from tensorflow.keras.layers import Resizing

def encoder_block(inputs, filters):
    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(inputs)
    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)
    p = MaxPooling2D((2, 2), padding='same')(x)  # Ensure max pooling does not reduce dimensions improperly
    return x, p

def conv_block(inputs, filters):
    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(inputs)
    x = Conv2D(filters, (3, 3), padding='same', activation='relu')(x)
    return x

def decoder_block(input_tensor, skip_tensor, filters):
    x = UpSampling2D((2, 2))(input_tensor)
    x = Concatenate()([x, skip_tensor])
    x = conv_block(x, filters)
    return x

def build_unet(input_shape=(400, 600, 3)):
    inputs = Input(input_shape)

    """ Encoder """
    s1, p1 = encoder_block(inputs, 64)
    s2, p2 = encoder_block(p1, 128)
    s3, p3 = encoder_block(p2, 256)
    s4, p4 = encoder_block(p3, 512)

    """ Bridge """
    b1 = conv_block(p4, 1024)

    """ Decoder """
    d1 = decoder_block(b1, s4, 512)
    d2 = decoder_block(d1, s3, 256)
    d3 = decoder_block(d2, s2, 128)
    d4 = decoder_block(d3, s1, 64)

    """ Output Layer """
    outputs = Conv2D(1, (1, 1), activation='sigmoid', padding='same')(d4)  # Ensure 'same' padding

    model = Model(inputs, outputs)
    return model




"""def build_unet(input_shape):
  inputs= Input(input_shape)

  Encoder

  s1, p1 = encoder_block(inputs, 64)
  s2, p2 = encoder_block(p1, 128)
  s3, p3 = encoder_block(p2, 256)
  s4, p4 = encoder_block(p3, 512)

  Bridge

  b1 = conv_block(p4, 1024)

  Decoder

  d1 = decoder_block(b1, s4, 512)
  d2 = decoder_block(d1, s3, 256)
  d3 = decoder_block(d2, s2, 128)
  d4 = decoder_block(d3, s1, 64)

  outputs = UpSampling2D(size=(101 // d4.shape[1], 101 // d4.shape[2]))(d4)

    # Final output layer
  outputs = Conv2D(1, (1, 1), activation='sigmoid')(outputs)
  return model"""

def load_data(path):
  train_x = sorted(glob(os.path.join(path, "train", "images", "*")))
  train_y = sorted(glob(os.path.join(path, "train", "masks", "*")))

  valid_x = sorted(glob(os.path.join(path, "valid", "images", "*")))
  valid_y = sorted(glob(os.path.join(path, "valid", "masks", "*")))

  return (train_x, train_y), (valid_x, valid_y)

def read_image(path):
  path = path.decode()
  x = cv2.imread(path, cv2.IMREAD_COLOR)
  x = x/255.0
  return x

def read_mask(path):
  path = path.decode()
  x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
  x = x/255.0
  x = np.expand_dims(x, axis=-1)
  return x

def tf_parse(x, y):
  def _parse(x, y):
    x = read_image(x)
    y = read_mask(y)
    return x, y

  x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])
  x.set_shape([height, width, 3])
  y.set_shape([height, width, 1])
  return x,y

def tf_dataset(x, y, batch=8):
  dataset = tf.data.Dataset.from_tensor_slices((x, y))
  dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.AUTOTUNE)
  dataset = dataset.batch(batch)
  dataset = dataset.prefetch(tf.data.AUTOTUNE)
  return dataset

(train_x, train_y), (valid_x, valid_y) = load_data(dataset_path)
print(f"Train: {len(train_x)} - {len(train_y)}")
print(f"Valid: {len(valid_x)} - {len(valid_y)}")

train_dataset = tf_dataset (train_x, train_y, batch=batch_size)
valid_dataset = tf_dataset (valid_x, valid_y, batch=batch_size)

for x,y in train_dataset:
  print(x.shape, y.shape)

from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, Cropping2D

def decoder_block(input_tensor, skip_tensor, filters):
    """ Decoder block with upsampling and concatenation """

    x = UpSampling2D((2, 2))(input_tensor)

    if x.shape[1] != skip_tensor.shape[1] or x.shape[2] != skip_tensor.shape[2]:
        if x.shape[1] > skip_tensor.shape[1] or x.shape[2] > skip_tensor.shape[2]:
            x = Cropping2D(((0, x.shape[1] - skip_tensor.shape[1]), (0, x.shape[2] - skip_tensor.shape[2])))(x)
        else:
            skip_tensor = Cropping2D(((0, skip_tensor.shape[1] - x.shape[1]), (0, skip_tensor.shape[2] - x.shape[2])))(skip_tensor)
    x = Concatenate()([x, skip_tensor])

    x = conv_block(x, filters)

    return x

input_shape = (height, width, 3)
model = build_unet(input_shape)

model.summary()

otp = tf.keras.optimizers.Adam(lr)
model.compile(loss="binary_crossentropy", optimizer=otp, metrics=["acc"])

callbacks=[
    ModelCheckpoint('files/non-aug/unet-non-aug.keras', verbose=1, save_best_only=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),
    CSVLogger(log_file),
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)
]

model.fit(train_dataset, epochs=epochs, validation_data=valid_dataset, callbacks=callbacks)

model.save('unet.keras')